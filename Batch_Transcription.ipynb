{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6efea47",
   "metadata": {},
   "source": [
    "# Async transcription with the Azure Speech service REST APIs and Python\n",
    "\n",
    "In this notebook, you'll learn how to read audio files from an Azure Blob Container, then use the Azure Speech service to transcribe the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19e4f9",
   "metadata": {},
   "source": [
    "## Before you get started\n",
    "\n",
    "* An [Azure subscription](https://azure.microsoft.com/en-us/free/cognitive-services/)\n",
    "* An [Azure Blob storage container](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal#create-a-container).\n",
    "  * **Note:** For this tutorial you can use mine\n",
    "  * **Note:** You'll need a second container if you want to write the transcripts to storage. Or you can just write them locally. Up to you.\n",
    "* An [Azure Speech service resource](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/overview#create-the-azure-resource) in the S0 pricing tier. This tutorial **won't** work with a *Free (F0)* key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194d759",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "\n",
    "The first thing we need to do is import a few modules. Here's what they are and what you'll use them for:\n",
    "1. `requests` - This module is used to make HTTP requests. Since Azure Blob storage and batch transcription are REST services, we'll be making a series of POST and GET requests to send and retrieve data from Azure.\n",
    "2. `xmltodict` - This modules quickly converts XML into a dictionary. From here we're going to turn the dictionary into JSON, which is a bit easier to use. **Note**: This is a personal preference, you can use ElementTree if XML is your jam.\n",
    "3. `json` - This module is used to encode and decode JSON. We'll use this module a fair amount in this guide.\n",
    "4. `time` - When we poll for results, we need to use the `time` module to add a delay to our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466acdf",
   "metadata": {},
   "source": [
    "## Get audio file URLs from Azure Blob storage\n",
    "\n",
    "This tutorial presumes that you have audio stored in an Azure Blob Container. If you have your own audio, you can replace this URL with your own, however, this container is publicly accessible and contains open source files created from Project Gutenberg and the Azure Text to Speech service.\n",
    "\n",
    "> **Important**: This container is public. It is set up this way for the tutorial. We recommend using the appropriate security measuures required for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6dcba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of containers in storage account\n",
    "response = requests.get('https://speechsamples21.blob.core.windows.net/audio-files-test-21/?comp=list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f7a73",
   "metadata": {},
   "source": [
    "## Convert XML to JSON and iterate over the response\n",
    "\n",
    "Here we're converting the XML returned by the Azure Blob storage REST API into a dictionary. Then we're encoding and deconding the JSON for use with the Speech service. \n",
    "\n",
    "> About halfway through this code block you'll get a printout of the JSONified response from Azure Blob storage. I'll also say, that if you're comfortable (or prefer) working with XML it won't offend me. Feel free to parse and iterate through the Azure Blob storage response directly â˜º. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13544863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the XML into a dict so we can convert it to JSON\n",
    "# This is a personal preference. If you prefer working with \n",
    "# XML you can use ElementTree.\n",
    "\n",
    "parsed_xml = xmltodict.parse(response.content)\n",
    "json_data = json.dumps(parsed_xml, indent=2)\n",
    "print(json_data)\n",
    "clean_json = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea53d22",
   "metadata": {},
   "source": [
    "Before we can call the Speech service, we need a URL for each audio file that we're transcribing. Here, we're going to loop through the JSONified response, pluck out the URLs, and add them to a list as we go. \n",
    "\n",
    "In a bit, we'll pass this list to the Speech service in our transcription request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of audio URLs we'll send to the Speech service\n",
    "audio_urls = []\n",
    "\n",
    "for i in clean_json:\n",
    "    for blob in clean_json[i]['Blobs']:\n",
    "        for audio_file in clean_json[i]['Blobs'][blob]:\n",
    "            audio_urls.append(audio_file['Url'])\n",
    "            print(f\"Audio added to list: {audio_file['Url']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a47998",
   "metadata": {},
   "source": [
    "## Create a transcription job\n",
    "\n",
    "Here we're sending a batch of audio files to the Azure Speech service to be transcribed. While our request is synchronous, the service will process each audio file asynchronously. \n",
    "\n",
    "Keep in mind, that the service will send a response almost immediately. However, the service may take up to a few minutes to transcribe your audio files depending on how many files you've sent and their size. The response contains a URL (possibly more than one) that we can use to fetch the status of our transcription request.\n",
    "\n",
    "Let's take a look at the request, review the response, and in the next section we'll discuss retrieving the job status and your transcriptions.\n",
    "\n",
    "* `region` - The regions for your Speech resource. For example: \"westus\"\n",
    "* `key` - The key for your Speech resource. \n",
    "* `displayName` - Give your transcription job a unique name. This will help you identify it if you run this more than once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key and region for Speech resource\n",
    "region = 'PASTE_YOUR_REGION_HERE'\n",
    "key = 'PASTE_YOUR_KEY_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "speech_base_url = f'https://{region}.api.cognitive.microsoft.com/speechtotext/v3.0/'\n",
    "\n",
    "# Operation\n",
    "operation = 'transcriptions'\n",
    "\n",
    "# Build the request\n",
    "request_headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "request_body = {\n",
    "    \"contentUrls\": audio_urls,\n",
    "    \"locale\": \"en-US\",\n",
    "    \"displayName\": \"GIVE_YOUR_JOB_A_NAME\"\n",
    "}\n",
    "\n",
    "request = requests.post(speech_base_url + operation, headers=request_headers, json=request_body)\n",
    "response = request.json()\n",
    "\n",
    "print(json.dumps(response, sort_keys=True, indent=4, ensure_ascii=False, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da165f9",
   "metadata": {},
   "source": [
    "## Get transcription status\n",
    "\n",
    "In the next two sections we're going to show you how to get the status of your transcription job and how to retrieve the transcriptions for each audio file. \n",
    "\n",
    "This specific operation will get the status for all transcription jobs that you've run that haven't been deleted (active and complete). You can call this specific API to determine if your transcription files are ready to be retrieved. \n",
    "\n",
    "<< TODO: Erik -- Fix this so that it looks for success of the specific job >> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "request = requests.get(speech_base_url + operation, headers=request_headers)\n",
    "response = request.json()\n",
    "\n",
    "# I need to clean this up to use the success param.\n",
    "if request.status_code == 200:\n",
    "    count = 0\n",
    "    while not response['values']:\n",
    "        print('Waiting for service. Trying again in 5 seconds.')\n",
    "        time.sleep(5)\n",
    "        count += 1\n",
    "        if count >= 6:\n",
    "            print(f'30 seconds have elapsed. Make sure that the previous request succeeded, then try again.')\n",
    "            break\n",
    "    else: \n",
    "        print(json.dumps(response, sort_keys=True, indent=4, ensure_ascii=False, separators=(',', ': ')))\n",
    "else:        \n",
    "    print(f'Status code: {request.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473bbe5",
   "metadata": {},
   "source": [
    "### Get transcriptions of your audio files\n",
    "\n",
    "To get your trancsriptions, you'll need the `['links']['files]` URL from either of these requests (which you've made previously):\n",
    "\n",
    "* Create transcription (POST)\n",
    "* Get transcriptions (GET)\n",
    "\n",
    "Replace `transcription_url` with the value of `['links']['files]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c43916",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_url = 'https://westus.api.cognitive.microsoft.com/speechtotext/v3.0/transcriptions/7f36020c-5135-41ab-b2be-4e9fbfc11fae/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "request = requests.get(transcription_url, headers=request_headers)\n",
    "response = request.json()\n",
    "if request.status_code == 200:\n",
    "    while not response['values']:\n",
    "        print('Waiting for service. Trying again in 5 seconds.')\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        print(json.dumps(response, sort_keys=True, indent=4, ensure_ascii=False, separators=(',', ': ')))\n",
    "else: \n",
    "    print(f'Status code {request.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79680e",
   "metadata": {},
   "source": [
    "## View a raw transcript\n",
    "\n",
    "In the previous section, the script printed the JSON response from the Speech service. Copy one of the `contentUrl`s and paste in the example below. \n",
    "\n",
    "When you run the script you'll get a raw print of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cef8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('REPLACE_ME').json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50975e2f",
   "metadata": {},
   "source": [
    "## Delete a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45139d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'REPLACE_WITH_A_SINGLE_ID'\n",
    "delete_id = f'/{id}'\n",
    "\n",
    "request_headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "request = requests.delete(speech_base_url + operation + delete_id , headers=request_headers)\n",
    "print(request.status_code, request.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68614dc",
   "metadata": {},
   "source": [
    "## Delete all jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_list = []\n",
    "\n",
    "request_headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "request = requests.get(speech_base_url + operation, headers=request_headers)\n",
    "response = request.json()\n",
    "\n",
    "for i in response['values']:\n",
    "    job_id_list.append(i['self'].replace(speech_base_url + operation + '/', ''))\n",
    "\n",
    "\n",
    "for job in job_id_list:\n",
    "    \n",
    "    job_id = f'/{job}'\n",
    "    \n",
    "    request_headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    request = requests.delete(speech_base_url + operation + job_id , headers=request_headers)\n",
    "    print(request.status_code, request.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3aaf7b",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "The Speech service v3 REST APIs allow you to do more than transcribe audio. They also allow you to create and manage custom speech models. To learn more, see the [Speech v3 REST API specification](https://westus.dev.cognitive.microsoft.com/docs/services/speech-to-text-api-v3-0/operations/GetTranscriptions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
